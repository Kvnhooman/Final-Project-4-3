{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "\n",
    "# Create a pandas DataFrame for the features and a pandas Series for the target variable\n",
    "df_features = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "s_target = pd.Series(boston.target, name='PRICE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "print(\"First 5 rows of the features DataFrame:\")\n",
    "print(df_features.head())\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset shape:\")\n",
    "print(df_features.shape)\n",
    "print(\"\\nDataset description:\")\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "We split the dataset into training and testing sets to evaluate the model's performance on unseen data. This helps prevent overfitting, where the model learns the training data too well but generalizes poorly to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, s_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "Feature scaling standardizes the range of independent variables or features of data. Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, many algorithms calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training features and transform both X_train and X_test\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize and train the LinearRegression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the scaled test data\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate and print the Mean Squared Error (MSE) and R-squared (R²) score\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"Linear Regression MSE: {mse_lr}\")\n",
    "print(f\"Linear Regression R²: {r2_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Results\n",
    "The Mean Squared Error (MSE) measures the average squared difference between the estimated values and the actual value. A lower MSE indicates a better fit.\n",
    "The R-squared (R²) score represents the proportion of the variance in the dependent variable that is predictable from the independent variables. An R² score closer to 1 indicates a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize and train the RandomForestRegressor model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the scaled test data\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate and print the Mean Squared Error (MSE) and R-squared (R²) score\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest Regressor MSE: {mse_rf}\")\n",
    "print(f\"Random Forest Regressor R²: {r2_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor Results\n",
    "The Random Forest Regressor is an ensemble learning method that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is generally more robust to overfitting than a single decision tree and can handle a large number of features.\n",
    "Comparing its MSE and R² score to Linear Regression can give insights into which model performs better on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Install xgboost if not already installed\n",
    "try:\n",
    "    import xgboost\n",
    "except ImportError:\n",
    "    print(\"Installing xgboost...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'xgboost'])\n",
    "    import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Initialize and train the XGBRegressor model\n",
    "xgb_model = XGBRegressor(random_state=42)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the scaled test data\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate and print the Mean Squared Error (MSE) and R-squared (R²) score\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost Regressor MSE: {mse_xgb}\")\n",
    "print(f\"XGBoost Regressor R²: {r2_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Regressor Results\n",
    "XGBoost (Extreme Gradient Boosting) is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting that solves many data science problems in a fast and accurate way. The same metrics (MSE and R²) are used for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Comparison\n",
    "\n",
    "| Model                   | Mean Squared Error (MSE) | R-squared (R²)   |\n",
    "|-------------------------|--------------------------|------------------|\n",
    "| Linear Regression       | {mse_lr:.4f}                 | {r2_lr:.4f}          |\n",
    "| Random Forest Regressor | {mse_rf:.4f}                 | {r2_rf:.4f}          |\n",
    "| XGBoost Regressor       | {mse_xgb:.4f}                | {r2_xgb:.4f}         |\n",
    "\n",
    "## Summary\n",
    "To evaluate the models, we look at MSE (lower is better) and R² (closer to 1 is better).\n",
    "\n",
    "Based on the scores:\n",
    "- The **XGBoost Regressor** typically performs the best, often yielding the lowest MSE and the highest R² score, indicating a strong predictive power and good fit to the data.\n",
    "- The **Random Forest Regressor** is also a strong performer and usually provides better results than Linear Regression. Its R² score is generally high, and MSE is lower than Linear Regression.\n",
    "- **Linear Regression** serves as a good baseline but is often outperformed by more complex ensemble methods like Random Forest and XGBoost, especially on datasets with non-linear relationships.\n",
    "\n",
    "*(Note: The actual best-performing model can vary depending on the dataset and specific hyperparameter tuning. The values in the table above will be populated when the notebook is executed.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Boosting Performance (Mother Section)**\n",
    "\n",
    "While the models above provide a good starting point, their performance can often be significantly improved. Here are several common techniques to boost model accuracy and robustness:\n",
    "\n",
    "## 1. Hyperparameter Tuning\n",
    "**Importance:** Most machine learning models have hyperparameters, which are settings that are not learned from the data but are set prior to training. The choice of hyperparameters can have a significant impact on model performance. Tuning these parameters is crucial for optimizing the model for a specific dataset.\n",
    "**Tools:** `sklearn.model_selection` provides tools like `GridSearchCV` and `RandomizedSearchCV`.\n",
    "   - `GridSearchCV`: Exhaustively searches over a specified parameter grid. It tries every combination of hyperparameter values and evaluates them using cross-validation.\n",
    "   - `RandomizedSearchCV`: Samples a given number of candidates from a parameter space with a specified distribution. It's often more efficient than `GridSearchCV`, especially when the hyperparameter space is large.\n",
    "\n",
    "## 2. Feature Engineering\n",
    "**Explanation:** This involves creating new features from existing ones or transforming existing features to better represent the underlying patterns in the data. Well-engineered features can lead to simpler models and improved performance.\n",
    "**Examples:**\n",
    "   - **Polynomial Features:** Creating features that are powers of existing features (e.g., x², x³). `sklearn.preprocessing.PolynomialFeatures` can be used for this. This can help capture non-linear relationships.\n",
    "   - **Interaction Terms:** Combining two or more features (e.g., featureA * featureB). This can capture how features jointly influence the target.\n",
    "   - **Domain-Specific Features:** Creating features based on expert knowledge of the problem domain. For example, in a housing dataset, creating a 'rooms_per_household' feature if 'total_rooms' and 'households' are available.\n",
    "\n",
    "## 3. Cross-Validation\n",
    "**Importance:** Cross-validation (CV) is a technique for assessing how the results of a statistical analysis will generalize to an independent dataset. It is crucial for robust model evaluation, especially when tuning hyperparameters, as it helps prevent overfitting by ensuring the model performs well on multiple subsets of the data.\n",
    "**K-Fold Cross-Validation:** A common method where the original training data is split into 'k' folds. For each fold, the model is trained on k-1 folds and validated on the remaining fold. The performance metric is then averaged across all k folds.\n",
    "\n",
    "## 4. Advanced Ensemble Methods\n",
    "**Explanation:** While Random Forest and XGBoost are powerful ensemble methods themselves, more advanced techniques can sometimes offer further improvements.\n",
    "   - **Stacking (Stacked Generalization):** Involves training multiple different models (base learners) and then using another model (meta-learner) to combine their predictions. The meta-learner is trained on the outputs of the base learners.\n",
    "   - It's worth noting that the complexity of implementing and tuning these methods can be higher.\n",
    "\n",
    "## 5. Feature Selection\n",
    "**Explanation:** Involves selecting a subset of the most relevant features from the original set. This can reduce model complexity, improve training time, reduce overfitting, and sometimes even improve performance by removing noise or irrelevant information.\n",
    "**Techniques:**\n",
    "   - **Recursive Feature Elimination (RFE):** Recursively removes features and builds a model on the remaining features.\n",
    "   - **Feature Importance Scores:** Many models (especially tree-based ones like Random Forest and XGBoost) provide feature importance scores. Features below a certain importance threshold can be removed.\n",
    "   - **Statistical Tests:** Using statistical tests (e.g., ANOVA F-value, chi-squared) to select features that have a strong relationship with the target variable.\n",
    "\n",
    "## 6. Handling Outliers and Data Cleaning\n",
    "**Explanation:** Outliers are data points that are significantly different from other observations. They can disproportionately affect model training and performance, especially for models sensitive to variance like Linear Regression. Further data cleaning beyond initial preprocessing might also be beneficial.\n",
    "**Actions:**\n",
    "   - **Outlier Detection and Treatment:** Techniques like using statistical methods (e.g., Z-score, IQR) or visualization to identify outliers. Treatment can involve removing them, transforming them (e.g., capping), or using robust models that are less sensitive to outliers.\n",
    "   - **Improved Imputation:** If missing values were handled simply (e.g., mean imputation), more sophisticated imputation techniques could be explored.\n",
    "   - **Data Transformation:** Applying transformations like log transforms to skewed data can sometimes help."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
